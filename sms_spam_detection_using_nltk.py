# -*- coding: utf-8 -*-
"""SMS Spam_Detection_Using NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9SWbB79hXU3F7EeWbYLPebJypu6RFjc

#*1. Importing all the libraries, NumPy, Seaborn, Pandas and Matplotlib*
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

#dataset_path - '/content/SMSSpamCollection'

#importing natural language toolkit
import nltk
from nltk import word_tokenize
import string, re  #regular expession
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.stem import LancasterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
nltk.download('wordnet')

# 1. Data Cleaning
# 2. EDA
# 3. Text Preprocessing
# 4. Model Building
# 5. Evaluation
# 6. Improvements
# 7. Websites
# 8. Deployments

#!unzip "/content/drive/MyDrive/Data-Science Edureka/Data Science New Project /Email Classification/SMS Spam/sms+spam+collection.zip"

"""### **2. Data Cleaning**"""

data = pd.read_csv("/content/SMSSpamCollection",  sep = "\t", header = None)
data.rename(columns =  {0:'Category', 1:"Email Text"}, inplace = True)  # rename columns name
data

data.info()
#Hence we can see that there in null objects present in the Category and Email, text

data.isnull().sum()
#null values are 0.
#data type is integer

data.duplicated().sum()

#It's is used to delete the same tuye of the value
data = data.drop_duplicates(keep = 'first')

#After deleting checking that if there is any duplicate value is available or not
data.duplicated().sum()

#Reseting the index after deleting and checking the present of duplicate value in the dataset
data = data.reset_index(drop = True)
data

#Checking the shape of the dataset  Rows*columns
data.shape

"""###**3. EDA - Exploratory Data Analysis**


"""

#Legitimate or desired message
print(f"Email Not Spam : {round(data['Category'].value_counts()[0] / len(data) * 100, 2)} %")
print(f"Spam Email : {round(data['Category'].value_counts()[1] / len(data) * 100 , 2)} %")

#Pie plot of the Spam and ham catogory
plt.pie(data['Category'].value_counts(),labels = ['ham', 'spam'], autopct = "%.2f%%");
plt.legend();

data['num_characters'] = data['Email Text'].apply(len)
data.head(10)

data['num_words'] = data['Email Text'].apply(lambda x : len(nltk.word_tokenize(x)))
#tokenize it into a list of word and calculate the length of that list
data.head()

data['num_sent'] = data['Email Text'].apply(lambda x : len(nltk.sent_tokenize(x)))
data.head()

data.describe()

#ham category describe
data[data['Category'] == 'ham'].describe()

#Spam category describe
data[data['Category'] == 'spam'].describe()

data.corr()
#resulting matrix show how strongly each variable is correlated with every oterh variable. The value range is from -1, 1

sns.heatmap(data.corr(), annot = True)  #correlation matrix of the datafrme

"""## **4. Data Preprocessing**

####Glimpse that how the stemmer and Lemmatization work
"""

import nltk
import spacy
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

words = ["eating", "eats", "eat", "ate", "adjustable", "rafting", "ability", "meeting"]
for word in words:
  print(word, "|", stemmer.stem(word))

"""###Lemmetization"""

nlp = spacy.load("en_core_web_sm")
doc = nlp("eating eats eat ate adjustable rafting ability meeting")
for token in doc:
    print(token, "|", token.lemma_)

#that why Lemmantizer is more sophisticated and correct.
#If we're taking about stemmer, it's only work on ing king of word, not in much correct details of the grammer.
#in case of Lammantizer we can see that it's linguistic based knowledge

port_stemmer = PorterStemmer()
lan_stemmer = LancasterStemmer()
lemmatizer = WordNetLemmatizer()

# Create a function to generate cleaned data from raw text
def clean_text(text):
    text = word_tokenize(text) # Create tokens
    text= " ".join(text) # Join tokens
    text = [char for char in text if char not in string.punctuation] # Remove punctuations
    text = ''.join(text) # Join the leters
    text = [char for char in text if char not in re.findall(r"[0-9]", text)] # Remove Numbers
    text = ''.join(text) # Join the leters
    text = [word.lower() for word in text.split() if word.lower() not in set(stopwords.words('english'))] # Remove common english words (I, you, we,...)
    text = ' '.join(text) # Join the leters
    # text = list(map(lambda x: lan_stemmer.stem(x), text.split()))
    text = list(map(lambda x: port_stemmer.stem(x), text.split()))
    # text = list(map(lambda x: lemmatizer.lemmatize(x), text.split()))
    return " ".join(text)   # error word

#cleaning the text

data['Clean Email'] = data['Email Text'].apply(clean_text)

data.columns  # printing the data column

data.head(4)

from wordcloud import WordCloud #Visually represent frequency of words in word cloud
wc = WordCloud(width = 2000, height = 1000, min_font_size = 10, background_color = 'Black')

spam_ = wc.generate(data[data['Category']=='spam']['Clean Email'].str.cat(sep = " "));
plt.figure(figsize = (10,6))
plt.imshow(spam_);

ham_ = wc.generate(data[data['Category']=='ham']['Clean Email'].str.cat(sep = " "));
plt.figure(figsize = (10, 8))
plt.imshow(ham_);

spam_word = []
for msg in data[data['Category'] == 'spam']['Clean Email'].tolist():
  for word in msg.split():
    spam_word.append(word)

len(spam_word)

from collections import Counter # Count- Occurance of  element in a collection
#ploting bar graph of the spam email
plt.bar(pd.DataFrame(Counter(spam_word).most_common(30))[0], pd.DataFrame(Counter(spam_word).most_common(30))[1])
plt.xticks(rotation = 'vertical');

ham_word = []
for msg in data[data['Category'] == 'ham']['Clean Email'].tolist():
  for word in msg.split():
    ham_word.append(word)

len(ham_word)

#Occurance of word in collection

from collections import Counter
#bar graph for the ham words
plt.bar(pd.DataFrame(Counter(ham_word).most_common(30))[0], pd.DataFrame(Counter(ham_word).most_common(30))[1])
plt.xticks(rotation = 'vertical');

from sklearn.preprocessing import LabelEncoder
#ecoding categorical label into numerical label values.
# Initialize the LabelEncoder
encoder = LabelEncoder()
# Fit and transform the labels
data['target'] = encoder.fit_transform(data['Category'])

"""### **5. Model Building**"""

#Tfidf-Term frequecy and inverse document frequency
from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer
cv = CountVectorizer

# Initialize the TF-IDF vectorizer
tf = TfidfVectorizer(max_features = 3000)

# Fit and transform the text column
X = tf.fit_transform(data['Clean Email']).toarray()
X.shape

y = data['target'].values
#Here x is a feature matrix and y is the target matrix

y

"""###**6.Training the model**"""



from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)
#It's used to split the data into train and test

"""###**7.Evaluation Phase**"""

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score, f1_score, roc_auc_score, mean_squared_error, r2_score, log_loss

gnb = GaussianNB() #It's commanly used when features are continuous
mnb = MultinomialNB()  #When features are discrete
bnb = BernoulliNB()  #suitable for binary/boolean feature

gnb.fit(X_train, y_train)  #gaussianNB
y_pred1 = gnb.predict(X_test)
print(f"Accuracy Score: {accuracy_score(y_test, y_pred1)}")
print(confusion_matrix(y_test, y_pred1))
print(f"Precision Score {precision_score(y_test, y_pred1)}")

mnb.fit(X_train, y_train) #multinominalNB
y_pred2 = mnb.predict(X_test)
print(f"Accuracy Score: {accuracy_score(y_test, y_pred2)}")
print(confusion_matrix(y_test, y_pred2))
print(f"Precision Score {precision_score(y_test, y_pred2)}")
#best accuracy

bnb.fit(X_train, y_train) #bernoulliNB
y_pred3 = bnb.predict(X_test)
print(f"Accuracy Score: {accuracy_score(y_test, y_pred3)}")
print(confusion_matrix(y_test, y_pred3))
print(f"Precision Score {precision_score(y_test, y_pred3)}")

import pickle
pickle.dump(tf, open('vectorizer.pkl', 'wb'))
pickle.dump(mnb, open('model.pkl', 'wb'))

sms = "“We’re happy to inform you that you’re entitled to a refund for overpayment on your AMEX account. Click on this link [Link] below to claim your refund.”"

tf_idf = pickle.load(open("/content/vectorizer.pkl", 'rb'))
model = pickle.load(open("/content/model.pkl", 'rb'))

#Preprocess
transformed_sms = clean_text(sms)
#Vectorize
vect_input = tf_idf.transform([transformed_sms])
result = model.predict(vect_input)[0] #predict

if result == 1:
  print("Spam")
else:
  print("Not Spam")

"""###**Predicting for the Multiple Model**"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

models = []
models.append(('Random Forest Classifier', RandomForestClassifier()))
models.append(('Logistic Regression', LogisticRegression()) )
models.append((('Decision Tree Classifier', DecisionTreeClassifier())))
models.append(('Naive bayes', MultinomialNB()))
models.append(('KNeighborsClassifier', KNeighborsClassifier()))
models.append(('Support Vector Machine', SVC()))

models

#Multiple Evaluation parameter
Model = []
Accuracy = []
Precision = []
Recall = []
F1 = []
for name, model in models:

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    Model.append(name)
    Accuracy.append(round(accuracy_score(y_test, y_pred), 4) * 100)
    Precision.append(round(precision_score(y_test, y_pred), 4) * 100)
    Recall.append(round(recall_score(y_test, y_pred), 4) * 100)
    F1.append(round(f1_score(y_test, y_pred), 4) * 100)

df = pd.DataFrame({'Model': Model, 'Accuracy':Accuracy, 'Precision':Precision, 'Recall':Recall, 'F1 Score':F1})

df = df.sort_values(by = 'Precision', ascending = False).reset_index(drop = True)
df



df = df.sort_values(by = 'F1 Score', ascending = False).reset_index(drop = True)
#All Evaluation of algo are present here in the same table

df

#Final visualization
plt.figure(figsize = (16,6))
plt.bar(df['Model'],df['F1 Score']);